!**************************************************************************************************                                    // !* by Hadi Zolfaghari, ARTORG Center, University of Bern, (zolfaghari.haadi@gmail.com)            *      
!* October 2015 - March 2020                                                                      *            
!* Modified by Hadi Zolfaghari, University of Cambridge (hz382@damtp.cam.ac.uk)                   *      
!* April 2020 -                                                                                   *            
!**************************************************************************************************

subroutine cuda_err_check(istat)

use iso_c_binding

! Module where we define CUDA functions to use them from Fortran
use mod_cudafor

implicit none

  integer, intent(in) :: istat

  ! Variables for error handling
  type(c_ptr) :: msg_ptr
  integer, parameter :: msg_len = 1024
  character(len=msg_len), pointer :: msg

  if (istat .ne. 0) then
      ! Get void* pointer to the error string
      msg_ptr = cudaGetErrorString(istat)
      ! Cast void* error string pointer to string
      call c_f_pointer(msg_ptr, msg)
      write(*, *) 'CUDA Error: ', &
          ! Take entire message string until first occurence of '\0' symbol
          ! -- end-of-string marker in C
          msg(1:index(msg, c_null_char))
      return
  endif

end subroutine cuda_err_check



subroutine device_gradient(dimension_id,phi,grad)

USE mod_dims
USE mod_vars
USE mod_vars_GPU
USE mod_exchange
USE mod_diff
USE mod_laplace
USE mod_inout




use iso_c_binding

! Module where we define CUDA functions to use them from Fortran
use mod_cudafor


implicit none


   integer, target, intent(in)    ::  dimension_id  ! the dimension_id of gradient --> corresponding to 'm=1,2,3' in IMPACT
   

   REAL(8)   , target, INTENT(inout) ::  phi (b1L:(N1+b1U),b2L:(N2+b2U),b3L:(N3+b3U))!< ? 
   REAL(8)   , target, INTENT(  out) ::  grad(b1L:(N1+b1U),b2L:(N2+b2U),b3L:(N3+b3U))!< gradient operator



  ! Declare the size of phi and other arguments passing to gradient -- ref : mod_diff

!   integer   ::   phi_size(3)
!   integer   ::   cGp1_size(2)
!   integer   ::   cGp2_size(2)
!   integer   ::   cGp3_size(2)



  ! Declare sincos kernel arguments with "target" -- for c_loc
!  integer, target     :: nx, ny, nz ! the grad and phi size components
!  integer, target     :: nx_grad_coef, ny_grad_coef !the cGp1 size components


!  type(c_ptr) :: grad_dev, phi_grad_dev, cGp1_dev, cGp2_dev, cGp3_dev
    
!  integer(c_int) :: istat
!
!  integer, parameter :: szblock = 1
!
!  ! CUDA constants
!  integer(c_int), parameter :: cudaMemcpyHostToDevice = 1
!  integer(c_int), parameter :: cudaMemcpyDeviceToHost = 2
!
!  integer(c_size_t) :: szreal, szint, szptr, offset = 0

  type(dim3) :: blocks, threads


!  type(cudaStream_t) :: stream1, stream2, stream3

  integer :: i, j, k, step
  real(8), volatile :: start, finish

  real(8)           :: t1,t2
  
  include 'mpif.h'


!! Aplying the exchange !!TODO this part is CPU only and a subject for shared memory optimization

   CALL exchange(dimension_id,0,phi) 

    ! Get the size of arrays passing to gradient subroutine
!    phi_size = shape(phi)

!    nx_dev = phi_size(1)  
!    ny_dev = phi_size(2) 
!    nz_dev = phi_size(3) 
 

!    szreal = sizeof(grad(1, 1, 1))
!    szint = sizeof(nx)
!    szptr = sizeof(grad_dev)
 
 
    

!    if (dimension_id .eq. 1)  cGp1_size = shape(cGp1)
!    if (dimension_id .eq. 2)  cGp2_size = shape(cGp2)
!    if (dimension_id .eq. 3)  cGp3_size = shape(cGp3)
!
!
!   if      (dimension_id .eq. 1) then
!        nx_grad_coef1 = cGp1_size(1)
!        ny_grad_coef1 = cGp1_size(2)
!    else if (dimension_id .eq. 2) then
!        nx_grad_coef2 = cGp2_size(1)
!        ny_grad_coef2 = cGp2_size(2)
!    else if (dimension_id .eq. 3) then
!        nx_grad_coef3 = cGp3_size(1)
!        ny_grad_coef3 = cGp3_size(2)
!    endif


!    print*, nx, ny, nz, nx_grad_coef, ny_grad_coef, 'nx, ny, nz, nxg, nyg', 'this is in gradient'
    ! Allocate memory on GPU.

!    if      (dimension_id .eq. 1) then
!        istat = cudaMalloc(cGp1_dev, nx_grad_coef1 * ny_grad_coef1 * szreal)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 2) then
!        istat = cudaMalloc(cGp2_dev, nx_grad_coef2 * ny_grad_coef2 * szreal)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 3) then
!        istat = cudaMalloc(cGp3_dev, nx_grad_coef3 * ny_grad_coef3 * szreal)
!        call cuda_err_check(istat)  
!    end if


!    istat = cudaMalloc(phi_grad_dev, nx_dev * ny_dev * nz_dev * szreal)
!    call cuda_err_check(istat)
!    istat = cudaMalloc(grad_dev, nx_dev * ny_dev * nz_dev *  szreal)
!    call cuda_err_check(istat)

	! XXX Several blocks of code below are equivalent to sum_kernel<<<...>>> in C.


!    print*, nx/8, ny/8, nz/8, 'block dimmension on gpu'
#ifdef CUDAVERSION9
    ! Create CUDA compute grid configuration.
    blocks = dim3((nx_dev-8)/8, (ny_dev-8)/8, (nz_dev-8)/8)
    if (mod(nx_dev, szblock) .ne. 0) blocks%x = blocks%x + 1
    threads = dim3((nx_dev-8)/blocks%x, (ny_dev-8)/blocks%y, (nz_dev-8)/blocks%z)
#endif
!!    blocks = dim3(1, 1, 1)
!!    threads = dim3(1, 1, 1)
    ! Copy input data from CPU to GPU memory.
!    if      (dimension_id .eq. 1) then
!        istat = cudaMemcpy(cGp1_dev, c_loc(cGp1), nx_grad_coef1 * ny_grad_coef1 * szreal, cudaMemcpyHostToDevice)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 2) then
!        istat = cudaMemcpy(cGp2_dev, c_loc(cGp2), nx_grad_coef2 * ny_grad_coef2 * szreal, cudaMemcpyHostToDevice)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 3) then
!        istat = cudaMemcpy(cGp3_dev, c_loc(cGp3), nx_grad_coef2 * ny_grad_coef2 * szreal, cudaMemcpyHostToDevice)
!        call cuda_err_check(istat)
!    end if
    


    istat = cudaMemcpy(phi_grad_dev, c_loc(phi), nx_dev * ny_dev * nz_dev * szreal, cudaMemcpyHostToDevice)
    call cuda_err_check(istat)

! Creating CUDA Streams  !TEST -- wrapper is buggy -->  Maybe c_loc(stream1) can help
!    istat = cudaStreamCreate(stream1)
!    call cuda_err_check(istat)
!    istat = cudaStreamCreate(stream2)
!    call cuda_err_check(istat)
!    istat = cudaStreamCreate(stream3)
!    call cuda_err_check(istat)




!    istat = cudaMemcpy(grad_dev, c_loc(grad), nx * ny * nz * szreal, cudaMemcpyHostToDevice)
!    call cuda_err_check(istat)


#ifdef CUDAVERSION9

!    print*, threads, blocks
!    print*, nx, ny, nz
   ! Setup CUDA compute grid configuration.
    istat = cudaConfigureCall(blocks, threads, int8(0), c_null_ptr)
    call cuda_err_check(istat)

!!! Stream 1 : the cuda API break down

    ! Setup CUDA kernel arguments (individually)
    offset = 0
    istat = cudaSetupScalarArgument(c_loc(dimension_id), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(dimension_id)
    istat = cudaSetupScalarArgument(c_loc(nx_dev), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(nx_dev)
    istat = cudaSetupScalarArgument(c_loc(ny_dev), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(ny_dev)
    istat = cudaSetupScalarArgument(c_loc(nz_dev), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(nz_dev)
     if      (dimension_id .eq. 1) then 
    istat = cudaSetupScalarArgument(c_loc(nx_grad_coef1), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(nx_grad_coef1)
    istat = cudaSetupScalarArgument(c_loc(ny_grad_coef1), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(ny_grad_coef1)
    else if      (dimension_id .eq. 2) then 
    istat = cudaSetupScalarArgument(c_loc(nx_grad_coef2), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(nx_grad_coef2)
    istat = cudaSetupScalarArgument(c_loc(ny_grad_coef2), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(ny_grad_coef2)
    else  if      (dimension_id .eq. 3) then 
    istat = cudaSetupScalarArgument(c_loc(nx_grad_coef3), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(nx_grad_coef3)
    istat = cudaSetupScalarArgument(c_loc(ny_grad_coef3), szint, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(ny_grad_coef3)
    end if




   ! Previous arguments were 4 bytes in size, total offset is 12 bytes.
    ! Since the next 3 arguments are 8 bytes each and have to be aligned
    ! by 8 bytes boundary, it's necessary to insert a 4-byte spacing here
    ! (from 12 to 16).
!    offset = offset + 4
    istat = cudaSetupArrayArgument(phi_grad_dev, szptr, offset)
    call cuda_err_check(istat)
    offset = offset + sizeof(phi_grad_dev)
    if      (dimension_id .eq. 1) then
        istat = cudaSetupArrayArgument(cGp1_dev, szptr, offset)
        call cuda_err_check(istat)
        offset = offset + sizeof(cGp1_dev)
    else if (dimension_id .eq. 2) then
        istat = cudaSetupArrayArgument(cGp2_dev, szptr, offset)
        call cuda_err_check(istat)
        offset = offset + sizeof(cGp2_dev)
    else if (dimension_id .eq. 3) then
        istat = cudaSetupArrayArgument(cGp3_dev, szptr, offset)
        call cuda_err_check(istat)
        offset = offset + sizeof(cGp3_dev)
    end if




    istat = cudaSetupArrayArgument(grad_dev, szptr, offset)
    call cuda_err_check(istat)

    ! Finally, launch CUDA kernel after we configured everything.
    ! Kernel is identified by C-string with its name (must be
    ! undecorated, i.e. with extern "C")

!    call MPI_Barrier(MPI_COMM_WORLD, merror)
!    t1 =   MPI_Wtime()

    istat = cudaLaunch('gradient_kernel' // c_null_char)
    call cuda_err_check(istat)

#endif
    if (dimension_id == 1) then
       call gradient_launcher((dimension_id), (nx_dev), (ny_dev),(nz_dev), (nx_grad_coef1), (ny_grad_coef1), (phi_grad_dev), (cGp1_dev), (grad_dev))
    elseif (dimension_id ==2) then
       call gradient_launcher((dimension_id), (nx_dev), (ny_dev),(nz_dev), (nx_grad_coef2), (ny_grad_coef2), (phi_grad_dev), (cGp2_dev), (grad_dev))
    elseif (dimension_id ==3) then
       call gradient_launcher((dimension_id), (nx_dev), (ny_dev),(nz_dev), (nx_grad_coef3), (ny_grad_coef3), (phi_grad_dev), (cGp3_dev), (grad_dev))
    endif 


!    call MPI_Barrier(MPI_COMM_WORLD, merror)
!    t2 =   MPI_Wtime()
!    print*, 'compute time: ', t2-t1
    ! Copy back results from GPU to CPU memory.
    istat = cudaMemcpy(c_loc(grad), grad_dev, nx_dev * ny_dev * nz_dev * szreal, cudaMemcpyDeviceToHost)
!    istat = cudaMemcpy(c_loc(pre), grad_dev, nx_dev * ny_dev * nz_dev * szreal, cudaMemcpyDeviceToHost)
    call cuda_err_check(istat)

    

    ! Free allocated GPU memory.
!    istat = cudaFree(grad_dev)
!    call cuda_err_check(istat)
!    istat = cudaFree(phi_grad_dev)
!    call cuda_err_check(istat)
!    if      (dimension_id .eq. 1) then
!        istat = cudaFree(cGp1_dev)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 2) then
!        istat = cudaFree(cGp2_dev)
!        call cuda_err_check(istat)
!    else if (dimension_id .eq. 3) then
!        istat = cudaFree(cGp3_dev)
!        call cuda_err_check(istat)
!    end if



end subroutine device_gradient

